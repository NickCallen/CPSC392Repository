{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split # simple TT split cv\n",
    "from sklearn.model_selection import KFold # k-fold cv\n",
    "from sklearn.model_selection import LeaveOneOut #LOO cv\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Together\n",
    "\n",
    "## Overview\n",
    "KNN is a simple, distance based algorithm that let's us CLASSIFY data points based on what class the data points around them are. Birds of a feather...\n",
    "\n",
    "Despite it being distance based, KNN is a *classification* algorithm. In other words, it is supervised machine learning, as it requires truth labels (the actual class/group). However it does share characteristics with clustering algorithms we will see later.\n",
    "\n",
    "KNN *can* work with binary/categorical variables, but not without some tweaking which we do not cover here.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "Hyperparameters are parameters in our model that are NOT chosen by the algorithm (we must supply them). We can either choose them:\n",
    "\n",
    "- based on domain expertise (knowledge about the data)\n",
    "- based on the data (hyperparameter tuning)\n",
    "\n",
    "Why do we have to use a validation set when hyperparameter tuning?\n",
    "\n",
    "\n",
    "## KNN plotting\n",
    "\n",
    "(this will only work with specific 2D data, if you wanted to use it for your own data you'd need to change the code to do so)\n",
    "\n",
    "K-Nearest Neighbors is a straightforward algorithm: given a training set, classify a new (unknown) data point by counting the K nearest known points, and choosing the most common classification.\n",
    "\n",
    "In this classwork we'll use ggplot to plot the boundaries of knn, and see how the size, shape, and overlap of clusters affect these boundries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotKNN2D(Xdf,y,k):\n",
    "    # X can only have 2 dimensions becuase of plotting\n",
    "    Xdf.columns = [\"x0\", \"x1\"]\n",
    "\n",
    "    #grab the range of features for each feature\n",
    "    x0_range = np.linspace(min(Xdf[\"x0\"]) - np.std(Xdf[\"x0\"]),\n",
    "                           max(Xdf[\"x0\"]) + np.std(Xdf[\"x0\"]), num = 100)\n",
    "    x1_range = np.linspace(min(Xdf[\"x1\"]) - np.std(Xdf[\"x1\"]),\n",
    "                           max(Xdf[\"x1\"]) + np.std(Xdf[\"x1\"]), num = 100)\n",
    "\n",
    "    #get all possible points on graph\n",
    "    x0 = np.repeat(x0_range,100)\n",
    "    x1 = np.tile(x1_range,100)\n",
    "    x_grid = pd.DataFrame({\"x0\": x0, \"x1\": x1})\n",
    "\n",
    "    #build model\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(Xdf,y)\n",
    "\n",
    "    # bredict all background points\n",
    "    p = knn.predict(x_grid)\n",
    "    x_grid[\"p\"] = p #add to dataframe\n",
    "    \n",
    "    #build the plot\n",
    "    bound = (ggplot(x_grid, aes(x = \"x0\", y = \"x1\", color = \"factor(p)\")) +\n",
    "                 geom_point(alpha = 0.2, size = 0.2) + theme_minimal() +\n",
    "                 scale_color_discrete(name = \"Class\") +\n",
    "                 geom_point(data = Xdf, mapping = aes(x = \"x0\", y = \"x1\", color = \"factor(y)\"), size = 2))\n",
    "    print(bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Fake Clusters\n",
    "\n",
    "## Explore\n",
    "\n",
    "We can use the sklearn function `make_blobs()` in order to generate fake groups of data. The `centers` variable stores the xy coordinates of the centers of each of our groups as tuples. The `cluster_std` variables stores the standard deviation for each cluster. `n` is the number of data points to be generated from the clusters.\n",
    "\n",
    "Changing `centers` changes how far apart the centers of the clusters are, whereas changing `cluster_std` changes how diffuse/spread out the clusters are. Try playing around with these numbers and using ggplot to plot the data (color by `factor(y)`). Notice how changing the numbers changes the layout of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### variables #################################################\n",
    "centers = [(-5, -5),\n",
    "           (5, 5)]\n",
    "cluster_std = [1, 1]\n",
    "n = 200\n",
    "### /variables #################################################\n",
    "\n",
    "X, y = make_blobs(n_samples=n, cluster_std=cluster_std,\n",
    "                  centers=centers, n_features=2, random_state=1)\n",
    "\n",
    "# make it into a dataframe for ggplot\n",
    "train = pd.DataFrame(X)\n",
    "train.columns = [\"x0\", \"x1\"]\n",
    "train[\"y\"] = y\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "# make a scatterplot of the data we just created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 How does changing k affect the decision boundary (imbalanced classes)?\n",
    "\n",
    "Now let's see how changing k affects the boundary when the groups have different numbers of samples. Using the `plotKNN2d()` function, and the `X` and `y` data generated below, examine what happens to the decision boundaries as you try different k's (try 1,3,5,10, 25, 50, and **100**).\n",
    "\n",
    "**How does changing k affect the decision boundary when the groups are imbalanced?**\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [(-2, -2),\n",
    "           (5, 5)]\n",
    "cluster_std = [6, 6]\n",
    "\n",
    "n1 = 50\n",
    "n2 = 150\n",
    "\n",
    "X, y = make_blobs(n_samples=[n1,n2], cluster_std=cluster_std,\n",
    "                  centers=centers, n_features=2, random_state=1)\n",
    "\n",
    "Xdf = pd.DataFrame(X)\n",
    "\n",
    "### MAKE A SCATTER PLOT OF THE DATA ### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1\n",
    "\n",
    "# k = 3\n",
    "\n",
    "# k = 5\n",
    "\n",
    "# k = 10\n",
    "\n",
    "# k = 25\n",
    "\n",
    "# k = 50\n",
    "\n",
    "# k = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "\n",
    "Use the telecom_churn.csv data from GitHub (for more information see this [link](https://www.kaggle.com/ivanhrek/telecom-churn)) and the KNN algorithm to predict `churn` in this dataset. Use TTS, and only continuous/interval variables. Z score your variables, and use `GridSearchCV()` to choose `k`.\n",
    "\n",
    "How accurate is your model? Is it just as good at predicting people who do not churn, as people who do churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN From Scratch\n",
    "\n",
    "Write a function, `neighbors()` that takes in three arguments:\n",
    "\n",
    "- `k`: the number of neighbors to find\n",
    "- `df`: a dataframe with ONLY continuous variables (can be any # of rows or columns)\n",
    "- `point`: the values of the data point you're finding neighbors for\n",
    "\n",
    "This function should find the euclidean distance between `point` and every other data point in `df` (hint: `np.linalg.norm()`, and return a list of the indices of the `k` nearest neighbors (by indices, I mean that if the k-nearest neighbors are in the 0th, 15th, 23rd, 32nd, and 56th rows, you should return a list `[0,15,23,32,56]`). Assume that the datapoint `point` is NOT included as a row in `df`.\n",
    "\n",
    "You may use `np.argpartition()` to find the indices of the `k` nearest neighbors. Below is an example of how it works. [Documentation](https://numpy.org/doc/stable/reference/generated/numpy.argpartition.html) linked here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 7]\n"
     ]
    }
   ],
   "source": [
    "# an array of numbers\n",
    "ar = [1, 7, 9, 2, 0.1, 17, 17, 1.5]\n",
    "\n",
    "# k (# of items)\n",
    "k = 3\n",
    "\n",
    "# get indices of k smallest values in ar\n",
    "indices = np.argpartition(ar, k)[:k]\n",
    "\n",
    "# check if this is correct\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neighbors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ebbd60404aaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# if your function is working, this should output `True`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'neighbors' is not defined"
     ]
    }
   ],
   "source": [
    "# test\n",
    "d = pd.DataFrame({\"x\" : [1.449, -1.069, -0.855, -0.281, -0.994, -0.969, -1.107, -1.252, -0.524, -0.497],\n",
    "                  \"y\" : [-1.806, -0.582, -1.109, -1.015, -0.162,  0.563,  1.648, -0.773,  1.606, -1.158]})\n",
    "\n",
    "p = np.array([-0.75, -1])\n",
    "\n",
    "k = 3\n",
    "\n",
    "# if your function is working, this should output `True`\n",
    "set(neighbors(k,d,p)) == set(np.array([3,2,9]))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
